\documentclass[a4paper, 11 pt]{article}
\usepackage{datetime}
\usepackage{lipsum}
\setlength{\columnsep}{25 pt}
\usepackage{tabularx,booktabs,caption}
\usepackage{multicol,tabularx,capt-of}
\usepackage{multirow}
\usepackage{subcaption}
\usepackage[utf8]{inputenc}
\usepackage{hyperref}
\usepackage{blkarray}
\usepackage[margin=1.2in]{geometry}
\usepackage{amsthm, amsmath, amssymb, amsfonts, commath, amsthm}
\usepackage{placeins}
\usepackage{graphicx}
\usepackage{mathrsfs}
\usepackage{listings}
\usepackage{lastpage}
\usepackage{enumerate}
\usepackage{subcaption}
\usepackage[danish]{babel}
%\renewcommand{\qedsymbol}{\textit{Q.E.D}}
\newtheorem{theorem}{Theorem}[section]
\addto\captionsdanish{\renewcommand\proofname{Proof}}
\newtheorem{lemma}{Lemma}[section]
\newtheorem{corollary}{corollary}[section]
\usepackage{fancyhdr}
\usepackage{titlesec}
\newcommand{\code}[1]{\texttt{#1}}
\lhead{Anders Gantzhorn - tzk942}
\chead{}
\rhead{09-11-2022}
\pagestyle{fancy}
\setlength{\headheight}{15pt}
\cfoot{\thepage\ af \pageref{LastPage}}
\setcounter{section}{0}
\title{Stochsatic gradient descent for logistic regression smoothing}
\author{Anders G. Kristensen}
\date{09-11-2022}
\begin{document}
\maketitle
\section{The log-logistic regression model}
\noindent We consider the four parameter log-logistic dose-response model of the real-valued random variable, $Y$ given $x$. The model is defined as
\[
    f(x|\alpha, \beta, \gamma, \rho) = \gamma + \frac{\rho-\gamma}{1+e^{\beta\log(x)-\alpha}}
\]
With $\alpha,\beta,\gamma,\rho\in\mathbb{R}$. We observe the noised variables
\[
    Y_i = f(x_i|\alpha,\beta,\gamma,\rho) + \varepsilon_i    
\]
for $i = 1,\dots, N$ and $\varepsilon_i\sim\mathcal{N}\left(0,\sigma^2\right)$ independent. We estimate $\left(\alpha,\beta,\gamma,\rho\right)\in\mathbb{R}^4$ by minimizing the non-linear least squares
\[
    H_N = \frac{1}{N}\sum_{i = 1}^N (y_i-f(x_i|\alpha,\beta,\gamma,\rho))^2 = \frac{1}{N}\sum_{i = 1}^N \left(y_i - \gamma + \frac{\gamma-\rho}{1+e^{\beta\log(x_i)-\alpha}}\right)^2
\]
We calculate the gradient of $H_N$ by computing the partial derivatives w.r.t the all the parameters.
\begin{align*}
    \frac{\partial H_N}{\partial \alpha}  &=  \frac{2}{N}\sum_{i = 1}^N \left(y_i-\gamma+\frac{\gamma-\rho}{1+e^{\beta\cdot\log(x_i)-\alpha}}\right)\frac{(\gamma-\rho)e^{\beta\log(x_i)-\alpha}}{\left(e^{\beta\log(x_i)-\alpha}+1\right)^2} \\
    \frac{\partial H_N}{\partial \beta}  &=  -\frac{2}{N}\sum_{i = 1}^N \left(y_i-\gamma+\frac{\gamma-\rho}{1+e^{\beta\cdot\log(x_i)-\alpha}}\right)\frac{(\gamma-\rho)\log(x_i)e^{\beta\log(x_i)-\alpha}}{\left(e^{\beta\log(x_i)-\alpha}+1\right)^2}\\
    \frac{\partial H_N}{\partial \gamma}  &=  \frac{2}{N}\sum_{i = 1}^N \left(y_i-\gamma+\frac{\gamma-\rho}{1+e^{\beta\cdot\log(x_i)-\alpha}}\right)\left(\frac{1}{1+e^{\beta\log(x_i)-\alpha}}-1\right)\\
    \frac{\partial H_N}{\partial \rho}  &=  -\frac{2}{N}\sum_{i = 1}^N \left(y_i-\gamma+\frac{\gamma-\rho}{1+e^{\beta\cdot\log(x_i)-\alpha}}\right)\left(\frac{1}{1+e^{\beta\log(x_i)-\alpha}}\right)
\end{align*}
\end{document}